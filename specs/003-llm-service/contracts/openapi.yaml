openapi: 3.0.0
info:
  title: LLM Service API
  version: 1.0.0
paths:
  /api/v1/llm/chat:
    post:
      summary: Generate chat completion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LLMRequest'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/LLMResponse'
            text/event-stream:
              description: Stream of LLMStreamChunk objects (if stream=true)
components:
  schemas:
    ChatMessage:
      type: object
      properties:
        role:
          type: string
          enum: [system, user, assistant]
        content:
          type: string
    LLMRequest:
      type: object
      required: [messages]
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatMessage'
        temperature:
          type: number
          format: float
        max_tokens:
          type: integer
        stream:
          type: boolean
    LLMResponse:
      type: object
      properties:
        content:
          type: string
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer
        finish_reason:
          type: string
